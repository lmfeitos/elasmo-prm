---
title: "Home Range Random Forest"
author: "Allie Caughman"
date: "1/10/2022"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(here)
library(tidymodels)
library(ranger)
library(patchwork)
library(DALEX)
library(kableExtra)

set.seed(42)
``` 

```{r, include=FALSE}
weighted.geomean <- function(x, w) {
  return(prod(x^w)^(1 / sum(w))) # calculates weighted geometric mean
}
```

```{r, include=FALSE}
## Read in all data
full_data <- read_csv(here("data", "randomforest_data_v2.csv")) %>% # load data
  mutate(movement_keyword = as.factor(movement_keyword)) %>% # turn categorical variables into factors
  mutate(family = as.factor(family)) %>% # turn categorical variables into factors
  select(species, family, homerange, n_ind, geog_range, movement_keyword, demers_pelag, length, common_name, kfin, r_fin, diet_troph3, Class, iucn_category) %>% # get relevant columns
  rename(hr = homerange) %>% # rename homerange
  filter(!is.na(r_fin)) %>% # remove columns with NA in predictors
  filter(!is.na(kfin)) %>% # remove columns with NA in predictors
  filter(!is.na(length)) %>% # remove columns with NA in predictors
  filter(!is.na(diet_troph3)) %>% # remove columns with NA in predictors
  filter(!is.na(movement_keyword)) %>% # remove columns with NA in predictors
  filter(!is.na(geog_range)) %>% # remove columns with NA in predictors
  distinct()

n_ind <- ifelse(!is.na(full_data$n_ind), full_data$n_ind, 1) # replace NA with 1
full_data$n_ind <- n_ind # add replaced column back to dataframe

# filter data to get rows with home range values
num <- full_data %>%
  group_by(species) %>% # group by species
  summarize(count = n()) %>% # get species counts
  filter(count > 1) # remove species with only one instance

ready_data <- full_data %>%
  filter(!(species %in% num$species)) %>% # get species with only 1 instance
  filter(!is.na(hr)) %>% # remove values where home range does not exist)
  select(species, hr) # select only species and home range

mean_calc <- full_data %>%
  filter(species %in% num$species) %>% # get species with more than 1 instance
  filter(!is.na(hr)) %>% # remove values where home range does not exist
  filter(species != "Centropomus undecimalis") %>% # remove species that geomean = infinity
  group_by(species) %>% # group by species
  summarize(hr = weighted.geomean(hr, n_ind)) # calculated weighted geometric mean

centropomus_undecimalis <- full_data %>%
  filter(species == "Centropomus undecimalis") %>% # calculate mean for species that geomean = infinity. Samples size for 2 occurrences are equal
  group_by(species) %>%
  summarise(hr = mean(hr))

hr_averages <- full_join(ready_data, mean_calc) # combine to get full observed home range data set

hr_averages <- full_join(hr_averages, centropomus_undecimalis) # combine to get full observed home range data set

# get list of species without home range values to be predicted
merge_data <- full_data %>%
  distinct(species, .keep_all = TRUE) %>% # keep one instance of each species
  select(-hr) # remove HR column

rf_data <- left_join(hr_averages, merge_data) # join averages with corresponding predictor data

predict_data <- full_data %>%
  select(-hr, -n_ind) %>%
  distinct(species, .keep_all = TRUE)
```


```{r, include=FALSE}
rf_split <- initial_split(rf_data, prop = .7, strata = family) # 70/30% split for training/testing

train <- training(rf_split) # make training split
test <- testing(rf_split) # make testing split

cv_folds <- vfold_cv(rf_data, repeats = 5, strata = family, v = 10) # folds for cross validation
```

### Overview

This model seeks to predict home range values for an array of fish species using 67 home range values found from the literature. Data were split into a testing and a training set and run through a random forest model using the ranger engine in R. The model is as follows:

home range ~ growth rate (r) + carrying capacity (K) + movement keyword + fish length + trophic level + geographic range

```{r, include=FALSE}
rf_recipe <- recipe(hr ~ r_fin + kfin + length + diet_troph3 + movement_keyword + geog_range, data = train) %>% # create prepossessing recipe
  step_normalize(all_numeric_predictors()) %>% # normalize all numeric variables
  step_log(all_outcomes(), skip = TRUE) # log transform outcome variable
```

```{r, include = FALSE}
rf_ranger <- rand_forest(trees = tune(), mtry = tune(), min_n = tune(), mode = "regression") %>% # set to tune all parameters with cross validation
  set_engine("ranger", verbose = TRUE, importance = "impurity", oob.error = TRUE, quantreg = TRUE) # set ranger regression engine
```

```{r, include = FALSE}
rf_wflw <- workflow() %>%
  add_model(rf_ranger) %>%
  add_recipe(rf_recipe) # create workflow with ranger model and preprocessed rf model
```

### Cross Validation

Cross validation will be used to choose the best hyperparameters for use in the model: 

* Number of trees
* Number of predictors in each sample
* Minimum data points in each node required for node split

```{r,include=FALSE}
rf_tune <- rf_wflw %>% # use cross validation to tune parameters
  tune_grid(
    resamples = cv_folds, # add cv folds
    grid = 10
  ) # use grid of parameters to test
```

Below displays the results of cross validation tuning

```{r}
autoplot(rf_tune) + # plot results of tuning
  theme_bw() # change theme
```

```{r}
select_best(rf_tune, metric = "rsq", n = 1) # get the best model
```

```{r, include=FALSE}
rf_final <- finalize_workflow(rf_wflw, select_best(rf_tune, metric = "rsq")) # finalize the workflow with the best model
```

```{r, include=FALSE}
rf_fit <- fit(rf_final, train) # fit the data to the training data

fit <- rf_fit$fit
```

```{r, include=FALSE}
train_predict <- predict(object = rf_fit, new_data = train) %>% # predict the training set
  bind_cols(train) %>% # bind training set column to prediction
  mutate(pred = exp(.pred)) %>% # back transform predictions
  mutate(log_hr = log(hr)) # get natural log of hr

test_predict <- predict(object = rf_fit, new_data = test) %>% # predict the training set
  bind_cols(test) %>% # bind prediction to testing data columns
  mutate(pred = exp(.pred)) %>% # back transform predictions
  mutate(log_hr = log(hr)) %>% # get natural log of hr
  mutate(common_name = test$common_name) %>% # add common names back
  mutate(species = test$species) %>% # add species name back
  mutate(family = test$family) # add families back

predict_full <- full_join(rf_data, predict_data) # combine predict data with training data

prediction <- predict(object = rf_fit, new_data = predict_full) %>% # predict the training set
  bind_cols(predict_full) %>% # bind prediction to full data
  mutate(pred = exp(.pred)) %>% # back transform predictions
  select(-.pred) %>% # remove unneeded column
  rename(observed_homerange = hr) %>% # rename columns
  rename(predicted_homerange = pred) # rename columns
```

Numeric predictor variables were normalized and and the home range values were log-transformed so that the large values did not over influence the results of the model. Root mean square error (RMSE) and $R^2$ were calculated on the test set to evaluate the model performance. Finally, the model was use to predict the home ranges for 600 additional species with unknown home range values.

```{r, include = FALSE}
train_metrics <- train_predict %>%
  metrics(log_hr, .pred) # get testing data metrics

test_metrics <- test_predict %>%
  metrics(log_hr, .pred) # get testing data metrics

test_metrics_back <- test_predict %>%
  metrics(hr, pred) # get testing data metrics

test_metrics_back_no_outlier <- test_predict %>%
  filter(species != "Thunnus albacares") %>%
  metrics(hr, pred) # get testing data metrics

rf_result <- test_predict %>%
  mutate(magitude_p = floor(log10(pred))) %>% # calculate the order of magnitude of the predictions
  mutate(magitude_hr = floor(log10(hr))) %>% # calculate the order of magnitude of the home ranges
  mutate(mag_diff = abs(magitude_p - magitude_hr)) # calculate difference in order of magnitude between prediction and home range

# write_csv(test_metrics, here::here("results", "hr_rf_metrics.csv"))
```

### Confidence Interval

In addition to metrics provided below, we calculated quantile intervals for the predictions at the 0.05, 0.25, 0.5, 0.75, and 0.95 level. They are based on quantiles are not true confidence intervals but provide a range of values for uncertainy estimation.

```{r, include=FALSE}
preds_bind <- function(data_fit) {
  predict(
    rf_fit$fit$fit$fit,
    workflows::extract_recipe(rf_fit) %>% bake(data_fit),
    type = "quantiles",
    quantiles = c(.5, .25, .5, .75, .95)
  ) %>%
    with(predictions) %>%
    as_tibble() %>%
    set_names(paste0(".pred", c("_05", "_25", "_50", "_75", "_95"))) %>%
    bind_cols(data_fit) %>%
    mutate(
      pred_50 = exp(.pred_50),
      pred_05 = exp(.pred_05),
      pred_95 = exp(.pred_95),
      pred_25 = exp(.pred_25),
      pred_75 = exp(.pred_75)
    )
} #function to preform and extract quantile estimates from reandom forest

quant_test <- preds_bind(test) %>%
  full_join(test_predict) #predict quantiles

quant_prediction <- preds_bind(predict_full) %>%
  full_join(prediction) %>% #add quantile predictions to prediction
  select(-.pred_50, -.pred_05, -.pred_95, -.pred_25, -.pred_75, -hr, -n_ind)

### Calculate percent of prediction quantiles that contain the observed value
quant_in_50 <- quant_prediction %>%
  mutate(in_interval = observed_homerange <= pred_75 & observed_homerange >= pred_25) %>%
  group_by(in_interval) %>%
  summarize(count = n())

percent_50 <- quant_in_50$count[2] / (quant_in_50$count[1] + quant_in_50$count[2]) * 100
percent_50

# write_csv(quant_prediction, here::here("data", "homerange_rf_predictions_v2.csv"))
```

### Metrics

First we calculated the testing and training RMSE and $R^2$ values. 
The training RSME was `r round(train_metrics$.estimate[1],2)` and the $R^2$ was `r round(train_metrics$.estimate[2],2)`.

Then cross validation was then preformed to get an average RMSE across folds

```{r, include = FALSE}
my_controls <- control_resamples(
  verbose = TRUE,
  save_pred = TRUE,
  extract = function(x) {
    model <- extract_fit_engine(x)
    model$prediction.error
  }
) #set to get prediction and out of bag errors

cv2 <- rf_final %>%
  fit_resamples(cv_folds, control = my_controls) # fit the folds to the final model
```

The results of cross validation are:

```{r}
collect_metrics(cv2)
```

The testing metrics are:

```{r, include=FALSE}
metric_table <- test_metrics %>% # create nice table of metrics
  as.data.frame() %>%
  kbl(digits = 4, col.names = c("Metric", "Estimator", "Estimate")) %>%
  kable_minimal()
```

```{r}
metric_table
```

A low RMSE indicates a good fit of the model to the data. However, it is not very interpretative in terms of understanding uncertainty in the model due to being on the log scale.

The next RMSE value is for the back transformed model

```{r, include=FALSE}
metric_table2 <- test_metrics_back %>% # create nice table of metrics
  as.data.frame() %>%
  kbl(digits = 4, col.names = c("Metric", "Estimator", "Estimate")) %>%
  kable_minimal()
```

```{r}
metric_table2
```

This is brought up highly by an outlier in the dataset. Due to the large variation in our dataset, this is also not easy to interpret. Without the outlier, the RMSE is 

```{r, include=FALSE}
metric_table3 <- test_metrics_back_no_outlier %>% # create nice table of metrics
  as.data.frame() %>%
  kbl(digits = 4, col.names = c("Metric", "Estimator", "Estimate")) %>%
  kable_minimal()
```

```{r}
metric_table3
```

This shows a very good model fit and an average difference between the predicted and actual value of 17.95 $km^2$. However, again, with a large spread of data, this still does not make sense for our data with very small values. Therefore, lastly, we seperated the data into 3 categories: low, medium, and high movement rates based on order of magnitude of the data and recalcuated RMSE values for each individual. 

```{r, include=FALSE}
rf_result <- rf_result %>%
  mutate(
    sq_error = (hr - pred)^2,
    abs_error = abs(hr - pred),
    sq_error_ln = (log_hr - .pred)^2,
    abs_error_ln = abs(log_hr - .pred)
  ) #calculate MSE for each value for use in dividing RMSE between movement groups

prediction <- prediction %>%
  mutate(magitude_p = floor(log10(predicted_homerange))) #calculate order of magnitude of prediction

high_test <- rf_result %>%
  filter(magitude_hr >= 0) %>%
  filter(species != "Thunnus albacares") #get high movement species

med_test <- rf_result %>%
  filter(magitude_hr < 0 & magitude_hr > -2) #get mid moving species

low_test <- rf_result %>%
  filter(magitude_hr <= -2) #get low movement species

high_rmse <- sqrt(mean(high_test$sq_error)) #RMSE for high movement species

med_rmse <- sqrt(mean(med_test$sq_error)) #RMSE for mid moving species

low_rmse <- sqrt(mean(low_test$sq_error)) #RMSE for low movement species
```

This resulted in and RMSE of 

```{r}
high_rmse
```

for the high movers (still excluding the outlier, the silky shark), an RMSE of 

```{r}
med_rmse
```

for the medium movers, and an RMSE of 

```{r}
low_rmse
```

For the low movers. These values are a bit more easily interpreted.

Next, we use this to get the out of bag error, which is the error for prediction of a sample that was not contained in the training set of a specific tree in the random forest. This helps to determine how well the random forest will preform of prediction samples not included in the model.

```{r, include=FALSE}
oob <- cv2 %>%
  collect_extracts() %>%
  unnest(.extracts) %>%
  mutate(rmse = sqrt(.extracts)) #get out of bag error
```

The average out of bag error is

```{r}
mean(oob$rmse) #average RMSE for out of bag predictions
```

While slightly higher than our test RMSE, this indicates the model preforms well on predictions.

### Diagonstics

#### Test accuracy of new prediction

Here are the predictions versus the actual values for the log-transformed model.

```{r, include=FALSE}
scatter_plot <- ggplot(test_predict, aes(x = log_hr, y = .pred)) + # plot ln of real versus ln of predicted
  geom_point() +
  geom_smooth(method = "lm") + # add trend line
  theme_bw() +
  labs(x = expression(ln(observed ~ home ~ range ~ (km^{
    "2"
  }))), y = expression(ln(predicted ~ home ~ range ~ (km^{
    "2"
  })))) +
  theme(text = element_text(size = 10))
```

```{r}
scatter_plot

# ggsave(scatter_plot, file = paste0("observed_vs_predicted_ln.png"), path = here::here("figs"))
```

```{r, include = FALSE}
lm <- lm(.pred ~ log(hr), data = test_predict) # linear regression of ln of real versus ln of predicted
```

```{r}
summary(lm) # get summary of the regression
```


```{r, include = FALSE}
back <- ggplot(test_predict, aes(hr, pred)) + # plot real versus predicted back transformed
  geom_point() +
  geom_smooth(method = "lm") + # get trend line
  theme_bw() +
  labs(x = "Observed Home Range km^2", y = "Predicted Home Range km^2")

back

# ggsave(back, file = paste0("observed_vs_predicted_ln_back.png"), path = here::here("figs"))
```

```{r, include=FALSE}
lm_t <- lm(pred ~ hr, data = test_predict) # linear regression real versus predicted back transformed
summary(lm_t) # get summary of back tranformed regression
```

#### Variable Imporance

```{r, include = FALSE}
hrs_test <- test$hr # extract HR for working with DALEX
test_vip <- test %>% # extract predictors for working with DALEX
  select(-Class, -common_name, -demers_pelag, -family, -iucn_category, -n_ind, -species, -hr)

expl <- explain(rf_fit, data = test_vip, y = hrs_test, predict_function = function(x, y) {
  predict(x, new_data = y) %>% pull(.pred)
}) # build DALEX explainer of the random forest model predicting test data

var_imp <- variable_importance(expl) %>%
  mutate(variable = fct_reorder(variable, desc(dropout_loss))) # use the DALEX explainer to get variable important

varimp <- ggplot(var_imp, aes(x = dropout_loss, y = variable)) + # plot variable importance
  geom_boxplot() +
  theme_bw() +
  labs(x = "Drop-out loss", y = "") +
  scale_y_discrete(name = "", labels = c("Baseline", "Geographic Range", "Length", "Carrying Capacity", "Trophic Level", "Movement Keyword", "Full Model", "Growth Rate")) +
  theme(text = element_text(size = 10))
```

```{r}
varimp

# ggsave(varimp, file = paste0("variable_importance_plot.png"), path = here::here("figs"))
```

Drop-out loss is the RMSE value for a model that includes on the variable of interest.

```{r, include=FALSE}
metric_plot <- scatter_plot + varimp + plot_annotation(tag_levels = "a")

# ggsave(metric_plot, file = paste0("fig5.pdf"), path = here::here("results"), width = 10, height = 5)
```

#### Distribution of real versus predicted home ranges for full data set

```{r, include = FALSE}
prediction_p <- ggplot(prediction, aes(predicted_homerange)) + # distribution of predicted values
  geom_histogram(bins = 30) +
  theme_bw() +
  labs(
    x = "Predicted Home Range (km^2)",
    y = "Count",
    title = "Predicted Home Ranges"
  )

real_p <- ggplot(rf_data, aes(hr)) + # distribution of observed values
  geom_histogram(bins = 30) +
  theme_bw() +
  labs(
    x = "Observed Home Range (km^2)",
    y = "Count",
    title = "Observed Home Ranges"
  )

dist_plot <- real_p / prediction_p

# ggsave(dist_plot, file = paste0("distribution_observed_vs_predicted.pdf"), path = here::here("results"))
# ggsave(dist_plot, file = paste0("distribution_observed_vs_predicted.png"), path = here::here("figs"))
```

```{r}
dist_plot
```

Predicted values match the distribution of the actual values well.

#### Orders of Magnitude

The realistic main goal of this model was to predict home range values within the correct order of magnitude. 

```{r, include= FALSE}
magnitude <- rf_result %>%
  group_by(mag_diff) %>% # group by the calculated difference in magnitude
  summarise(count = n()) %>% # get number of predictions in each category
  mutate(percent = count / nrow(test) * 100) # get percentage
```

```{r, include=FALSE}
mag_table <- magnitude %>%
  kbl(digits = 2, col.names = c("Order of Magnitude", "Count", "Percent")) %>%
  kable_minimal()
```

```{r}
mag_table
```

Most values in the testing set were predicted within 1 order of magnitude of the correct value, indicating the model is useful at predicting on the correct order of magnitude.

```{r, include=FALSE}
# write_csv(magnitude, here::here("results", "magnitude_difference_summary.csv"))
```

```{r, include=FALSE}
mean(rf_result$mag_diff) # mean diff in magnitude
median(rf_result$mag_diff) # median diff in magnitude
```
